# üöÄ FastGPT - Chatbot Moderno con Ollama

Un chatbot web moderno y elegante que conecta con Ollama para proporcionar una experiencia de chat inteligente desde cualquier dispositivo.

![FastGPT Preview](https://img.shields.io/badge/FastGPT-v2.0-blue?style=for-the-badge&logo=robot)

## ‚ú® Caracter√≠sticas Principales

### üé® Interfaz Moderna
- **Dise√±o responsive** que se adapta perfectamente a m√≥viles, tablets y desktop
- **Tema elegante** con gradientes y animaciones suaves
- **Sidebar deslizante** para navegaci√≥n de chats
- **Indicadores de estado** en tiempo real

### ü§ñ Funcionalidades de Chat
- **M√∫ltiples conversaciones** con historial persistente
- **Selector de modelos** din√°mico (Llama 3.1, Qwen 2.5, Mistral, etc.)
- **Contexto inteligente** que mantiene la coherencia en la conversaci√≥n
- **Animaciones de escritura** que simulan respuestas naturales
- **Timestamps** en cada mensaje

### üîß Optimizaciones T√©cnicas
- **API as√≠ncrona** con FastAPI para m√°ximo rendimiento
- **Manejo inteligente de errores** y reintentos autom√°ticos
- **Almacenamiento local** del historial de chats
- **Verificaci√≥n de estado** de Ollama en tiempo real
- **Soporte para streaming** de respuestas (futuro)

### üì± Acceso Remoto
- **Servidor configurado** para acceso desde cualquier dispositivo en la red local
- **CORS habilitado** para desarrollo y uso m√≥vil
- **URLs amigables** para acceso f√°cil desde el tel√©fono

## üõ†Ô∏è Instalaci√≥n y Configuraci√≥n

### Prerrequisitos
1. **Python 3.8+** instalado
2. **Ollama** instalado y ejecut√°ndose
3. Al menos un modelo descargado en Ollama

### Instalaci√≥n R√°pida

1. **Instala las dependencias:**
   ```bash
   cd C:\Users\Lightning\Documents\FastGPT
   pip install fastapi uvicorn httpx requests
   ```

2. **Verifica que Ollama est√© ejecut√°ndose:**
   ```bash
   ollama serve
   ```

3. **Descarga algunos modelos (opcional):**
   ```bash
   ollama pull llama3.1:8b
   ollama pull qwen2.5:7b
   ollama pull mistral:7b
   ```

4. **Ejecuta el servidor:**
   ```bash
   # Opci√≥n A: Script autom√°tico (recomendado)
   start-backend.bat
   
   # Opci√≥n B: Manual
   cd backend
   python server.py
   ```

5. **Accede al chatbot:**
   - **Local:** http://localhost:8000
   - **M√≥vil:** http://192.168.1.64:8000 (o tu IP local)

## üåç Deployment P√∫blico (¬°GRATIS!)

¬øQuieres acceder a tu chatbot desde cualquier lugar del mundo? **[Consulta la Gu√≠a Completa de Deployment](DEPLOYMENT_GUIDE.md)**

### üöÄ Lo que Obtienes:
- üîó **URL personalizada gratuita:** `tu-chatbot.pages.dev`
- üåê **Acceso global:** Desde cualquier pa√≠s y dispositivo
- üì± **PWA optimizado:** Funciona como una app nativa
- ‚ö° **CDN global:** Velocidad m√°xima en todo el mundo
- üîí **HTTPS autom√°tico:** Certificados SSL gratis
- üí∞ **Costo:** ¬°**COMPLETAMENTE GRATIS**!

### ‚ö° Inicio R√°pido:
1. Copia el archivo `frontend/index.html`
2. Configura tu backend con ngrok: `ngrok http 8000`
3. Sube el frontend a Cloudflare Pages
4. ¬°Listo! Tu chatbot ya es accesible desde cualquier lugar

## üìÇ Estructura del Proyecto

```
FastGPT/
‚îú‚îÄ‚îÄ üìÅ backend/                 # Servidor Python FastAPI
‚îÇ   ‚îú‚îÄ‚îÄ server.py              # Servidor principal
‚îÇ   ‚îî‚îÄ‚îÄ static/
‚îÇ       ‚îî‚îÄ‚îÄ index.html         # Interfaz local simple
‚îú‚îÄ‚îÄ üìÅ frontend/               # Frontend para deploy
‚îÇ   ‚îî‚îÄ‚îÄ index.html             # Interfaz completa para Cloudflare
‚îú‚îÄ‚îÄ üöÄ start-backend.bat       # Script autom√°tico de inicio
‚îú‚îÄ‚îÄ üìò DEPLOYMENT_GUIDE.md     # Gu√≠a de despliegue completa
‚îú‚îÄ‚îÄ üìñ README.md               # Esta documentaci√≥n
‚îî‚îÄ‚îÄ üìÅ venv/                   # Entorno virtual Python
```

## üéØ Caracter√≠sticas Detalladas

### üîÑ Sistema de Chat Inteligente
- **Historial persistente:** Los chats se guardan autom√°ticamente en localStorage
- **Cambio de modelos:** Selecciona entre diferentes modelos de IA sin reiniciar
- **Contexto din√°mico:** El sistema mantiene el contexto de la conversaci√≥n autom√°ticamente
- **T√≠tulos autom√°ticos:** Los chats se titulan autom√°ticamente basados en el primer mensaje

### üé® Experiencia de Usuario
- **Animaciones fluidas:** Transiciones suaves en toda la interfaz
- **Indicadores visuales:** Estados de conexi√≥n, escritura y carga claramente visibles
- **Responsive design:** Funciona perfectamente en todos los dispositivos
- **Atajos de teclado:** Enter para enviar, Shift+Enter para nueva l√≠nea

### ‚ö° Rendimiento Optimizado
- **API as√≠ncrona:** Manejo no bloqueante de m√∫ltiples solicitudes
- **Timeouts inteligentes:** Configuraciones optimizadas para diferentes operaciones
- **Reintentos autom√°ticos:** Sistema robusto de manejo de errores
- **Carga lazy:** Los recursos se cargan solo cuando son necesarios

## üîß Configuraci√≥n Avanzada

### Modificar el archivo `config.json`:

```json
{
    "server": {
        "host": "0.0.0.0",        // IP del servidor
        "port": 8000,             // Puerto del servidor
        "reload": false           // Hot reload (solo desarrollo)
    },
    "ollama": {
        "url": "http://localhost:11434", // URL de Ollama
        "timeout": 120,                   // Timeout en segundos
        "max_retries": 3                  // Reintentos m√°ximos
    },
    "chat": {
        "max_context_messages": 10,      // Mensajes de contexto m√°ximos
        "default_temperature": 0.7,      // Creatividad del modelo
        "default_max_tokens": 2000       // Longitud m√°xima de respuesta
    }
}
```

## üåê Endpoints de la API

### Chat
- **POST** `/api/chat` - Enviar mensaje
- **POST** `/api/chat/stream` - Chat con streaming (pr√≥ximamente)

### Modelos
- **GET** `/api/models` - Lista de modelos disponibles
- **GET** `/api/status` - Estado de conexi√≥n con Ollama

### Sistema
- **GET** `/health` - Estado del servidor
- **GET** `/` - Interfaz web principal

## üì± Uso en M√≥viles

### Configuraci√≥n de Red
1. Aseg√∫rate de que tu PC y m√≥vil est√°n en la misma red WiFi
2. Verifica tu IP local: `ipconfig` (Windows) o `ip addr` (Linux)
3. Accede desde el m√≥vil usando: `http://[TU_IP]:8000`

### Funciones M√≥viles
- **Sidebar t√°ctil:** Desliza para abrir/cerrar la lista de chats
- **Textarea adaptable:** Se ajusta autom√°ticamente al contenido
- **Botones optimizados:** Tama√±o perfecto para dedos
- **Navegaci√≥n intuitiva:** Gestos naturales en toda la interfaz

## üöÄ Mejoras Implementadas

### Backend Optimizado
- ‚úÖ **API as√≠ncrona** con FastAPI
- ‚úÖ **Manejo de contexto** inteligente
- ‚úÖ **Sistema de reintentos** robusto
- ‚úÖ **Endpoints adicionales** para modelos y estado
- ‚úÖ **Configuraci√≥n flexible** de modelos

### Frontend Moderno
- ‚úÖ **Dise√±o completamente renovado** con CSS moderno
- ‚úÖ **JavaScript orientado a objetos** y modular
- ‚úÖ **Sistema de estado** robusto
- ‚úÖ **Animaciones fluidas** y naturales
- ‚úÖ **Responsive design** perfecto

### Experiencia de Usuario
- ‚úÖ **Historial persistente** con localStorage
- ‚úÖ **Selector de modelos** din√°mico
- ‚úÖ **Indicadores de estado** en tiempo real
- ‚úÖ **Animaciones de escritura** realistas
- ‚úÖ **Manejo de errores** amigable

## üêõ Soluci√≥n de Problemas

### Error de Conexi√≥n con Ollama
```bash
# Verifica que Ollama est√© ejecut√°ndose
ollama serve

# Verifica que los modelos est√©n disponibles
ollama list
```

### El servidor no arranca
```bash
# Verifica que las dependencias est√©n instaladas
pip install fastapi uvicorn httpx requests

# Verifica que el puerto no est√© en uso
netstat -ano | findstr :8000
```

### No se puede acceder desde el m√≥vil
1. Verifica la IP con `ipconfig`
2. Aseg√∫rate de que Windows Firewall permita el puerto 8000
3. Confirma que est√©s en la misma red WiFi

## üìà Pr√≥ximas Caracter√≠sticas

- üîÑ **Streaming de respuestas** en tiempo real
- üé® **Temas personalizables** (oscuro/claro)
- üìÅ **Exportar/importar** chats
- üîç **B√∫squeda en historial** de conversaciones
- üîí **Autenticaci√≥n** y usuarios m√∫ltiples
- üìä **Estad√≠sticas de uso** y analytics
- üåç **M√∫ltiples idiomas** (i18n)

## ü§ù Contribuci√≥n

¬øQuieres contribuir? ¬°Excelente!

1. Fork del proyecto
2. Crea una rama para tu feature
3. Commit tus cambios
4. Push a la rama
5. Abre un Pull Request

## üìÑ Licencia

Este proyecto est√° bajo la Licencia MIT. Consulta el archivo `LICENSE` para m√°s detalles.

## üôã‚Äç‚ôÇÔ∏è Soporte

Si tienes preguntas o problemas:

1. Revisa la secci√≥n de **Soluci√≥n de Problemas**
2. Verifica que Ollama est√© ejecut√°ndose correctamente
3. Aseg√∫rate de tener las dependencias instaladas

---

**¬°Disfruta usando FastGPT! üéâ**

> Hecho con ‚ù§Ô∏è para proporcionar la mejor experiencia de chat con IA local.
